\section{\label{appendix:JAX}JAX NumPy: Usage Tutorial}

JAX is a Python library developed by Google that provides an easy and efficient way to perform numerical computations, particularly those involving large matrices and tensors, on a GPU. JAX uses a technique called \textit{Just-In-Time (JIT) compilation}, which compiles the Python code into optimized machine code that can run on the GPU. This allows JAX to take advantage of the parallelism offered by the GPU, making computations much faster than if they were performed on the CPU.

One of the key features of JAX is its ability to automatically differentiate functions. JAX also provides a set of linear algebra operations that are optimized for GPU acceleration, including matrix multiplication, matrix-vector multiplication, and matrix inversion. To perform matrix operations using JAX, one first needs to create arrays or tensors that contain the matrix data and then use the JAX linear algebra functions to perform matrix operations on these arrays. 

The usage of the JAX NumPy library involves a minor change in standard NumPy Python code by replacing most NumPy function calls with JAX NumPy calls. For example, to define a 2-D tensor of zeroes of shape $(5,3)$ in JAX NumPy, we would do the following:

\begin{verbatim}
    import jax.numpy as jnp
    new_arr = jnp.zeros((5,3))
\end{verbatim}

To use the JIT feature of JAX NumPy, we can think of JIT as a decorator that accepts functions operating on JAX arrays and their arguments as static arguments. For example, if we have a function 'addJAX' defined over JAX NumPy arrays, we would tell the Python interpreter to use JIT compilation for this function as follows:

\begin{verbatim}
    from jax import jit
    add_jit = jit(addJAX)
\end{verbatim}

JAX NumPy does not allow tensor slicing or direct indexing but rather makes use of the C++ style `at' function to access tensor elements by index. In addition, JAX tensors are immutable, i.e. they cannot be edited in place. Please refer to the official documentation of JAX NumPy for further details on JAX methods at \url{https://jax.readthedocs.io/en/latest/jax.numpy.html}


\section{\label{appendix:GPU-acc-comparison}Comparison of different off-the-shelf GPU-based tensor manipulation libraries}

For the centralized joint trajectory optimization problem for multiple holonomic robots, we implement the path planning algorithm using three different off-the-shelf open-source Python libraries - vanilla NumPy, JAX NumPy, and CuPy. The latter two libraries use accelerated tensor computations to achieve computational speed-up in computing large tensor operations, such as matrix multiplications, inverses, and so on. As discussed in Appendix \ref{appendix:JAX}, JAX NumPy leverages GPU acceleration for CUDA-based tensor computations, and so does CuPy. Table \ref{table: gpu-acc-comp} compares the time taken to plan start-to-goal offline trajectories for a varying number of robots using the same core centralized trajectory optimization approach.

\begin{table}
\centering
\caption{Comparison of multi-agent path planning times using different mathematical libraries} \label{table: gpu-acc-comp}
\scriptsize
\begin{tabular}{|p{3cm}|p{3cm}|p{3cm}|}\hline
 Number of Robots & Planning time(in sec) & Library
\\ \hline
4 & 0.0050 & NumPy
\\ \hline
8 & 0.0049 & NumPy
\\ \hline
16 & 0.0046 & CuPy
\\ \hline
32 & 0.0046 & JAX NumPy
\\ \hline
32 & 0.267 & NumPy
\\ \hline
32 & 0.0049 & CuPy
\\ \hline
64 & 0.0047 & JAX NumPy
\\ \hline
\end{tabular}
\normalsize
\vspace{-0.4cm}
\end{table}

We observe from Table \ref{table: gpu-acc-comp} that compared to NumPy, JAX NumPy achieves a computational acceleration of around $60$ times over vanilla NumPy by leveraging GPU computations. Further, the planning time for 32 as well as 64 robots are similar, indicating that the JAX-based implementation of the algorithm remains nearly constant regardless of the increase in the number of robots, or, in other words, an increase in the size of the matrices used in the optimization process. For CuPy, we observe a similar trend in the planning time, where between 16 and 32 robots, the computation time remains nearly constant as well. 
